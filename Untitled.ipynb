{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron \n",
    "\n",
    "*Perceptron* is a supervised learning algorithm for binary classifiers. It is supervised because we can train this algorithm from a training dataset and test it out on the testing data. It gives results in a linear function. Perceptrons are just a single layer Neural Network. If we add many perceptrons taking care of different functionalities, we can create a multi layer Perceptron. In the early days, it was difficult to classify some problems in as a linear function. So they were not considered as a serious solution to Artificial Intelligence. But, when people experimented with adding more layers to the perceptron, they found that problems which cannot be classified as a linear function, can now be solved. One example of this is the **XOR** solution. This XOR problem cannot be implemented in a single layer of perceptron as the results are not linearly separable. Therefore, people tried to implement multi-layer perceptron which got the required result. Hence, it was the advent of Perceptron and Artificial Intelligence.\n",
    "\n",
    "In this algorithm, we create a feature function. This function takes features from the image according to the function specified. *For our implementation*, we ask the user to input the number of grids they want to compute on. In general, if the grid size is greater than or equal to 3\\*3, the accuracy is higher. Also, we take input from the user the dataset to run these algorithms on. Users can enter **FACE** or **DIGIT** to run the algorithm on face or digit datasets respectively.\n",
    "\n",
    "Initially, we take random weights for features of each label and compute the score of feature of a class(for digits, 6 or 9 & for face, face or no face). It can be mathematically be written as $score(f,y) = \\sum_i f_i w_i^y$. This equation means we calculate the score for feature vector *f* for a particular class *y* which equals to sum of the multiplication of all the features vectors with its weight of its class. \n",
    "\n",
    "We update the weights by calculating the maximum of the the score it gives for a label from the individual features. We can write it as $ y^{*} = arg max score(f,y^{**}) $ . Then after computing each feature and checking for the actual label, we predict if the result is accurate. For example, if the label is *6*, and we get a prediction of *9*, then, in this case, we decrease the values of weights for the label 9 as $w^9 = w^9 - f$ where f is some predefined value which we will subtract from the current weight. But for the weights of 6, we will add f to its current weight as $w^6 = w^6 + f$. Finally, the perceptron algorithm would tune the weight according to the actual label during the training and try to get the accurate result in the testing data.\n",
    "\n",
    "**Advantages**\n",
    "- Perceptrons have the ability to learn themselves the complex relationships and patterns in the dataset.\n",
    "- We can have any type of input. It does not restrict to use any one datatype as inputs.\n",
    "- If we have a single layer of perceptron, then the training is very quick.\n",
    "- Is really accurate for image processing and character recognition.\n",
    "\n",
    "**Disadvantages**\n",
    "- A single layer of perceptron cannot train a problem whose solution is a non-linear function.\n",
    "- Multi layer perceptron takes more time to train.\n",
    "- Difficult optimization if we have a lot of local minima/maxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PerceptronClassifier:\n",
    "    def __init__(self, FEATURES, LABELS):\n",
    "        self.SHAPE = (LABELS, FEATURES + 1)  # The +1 is for our w0 weight.\n",
    "        self.weightMatrix = np.zeros(self.SHAPE)\n",
    "\n",
    "    def updateWeights(self, predictedLabel, actualLabel, featureValueList):\n",
    "        # print(\"Updating Weights\")\n",
    "        self.weightMatrix[actualLabel] = self.weightMatrix[actualLabel] + featureValueList\n",
    "        self.weightMatrix[predictedLabel] = self.weightMatrix[predictedLabel] - featureValueList\n",
    "        # print(weightMatrix[actualLabel, :])\n",
    "        # print(weightMatrix[predictedLabel, :])\n",
    "\n",
    "    def runModel(self, isTrain, featureValueList, actualLabel):\n",
    "        featureScoreList = []\n",
    "        featureValueList = [1] + featureValueList # The [1] + is to accommodate the bias weight - w0\n",
    "        for labelWeights in self.weightMatrix:\n",
    "            featureScoreList.append(np.sum(np.dot(labelWeights, featureValueList)))\n",
    "\n",
    "        # print(\"Feature Score List :\", featureScoreList)\n",
    "        predictedLabel = np.argmax(featureScoreList)\n",
    "\n",
    "        if predictedLabel != actualLabel:\n",
    "            # print(predictedLabel, \" \", actualLabel)\n",
    "            if isTrain:\n",
    "                self.updateWeights(predictedLabel, actualLabel, featureValueList)\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def initWeightMatrix(self):\n",
    "        self.weightMatrix = np.zeros(self.SHAPE)  # Randomized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "*Naive Bayes Classifier* is a part of the probabilistic classifiers based on Bayes' Theorem. The formula for Bayes' theorem is $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$. There are two assumptions taken in this algorithm.\n",
    "- That the feature is independent.\n",
    "- The importance of every feature is equal.\n",
    "When we have a label *X*, the naive bayes tries to model it using joint distribution from the formula mentioned above. For every labels, we have features dedicated for them as a group like $ (f_1, f_2, ..., f_n) $. We have a  formula given for calculating the joing probability. $ (f_1, f_2, ..., f_n, X) = P(X) \\prod_i P(F_i|X) $. As we had done in the perceptron algorithm, we again calculate the argmax of the features from the probabilities and predict the label from the set of inputs given. \n",
    "\n",
    "Another thing for the prediction is smoothing. In real life, we do not want to set the probabilities of any term to be 0. But, if the algorithm does not see any connections between $A$ and $B$ in the formula, it would give $P(A|B) = 0$. This is not acceptable in real world predications and as a result, we use *Laplace Smoothing* to get rid of any probabilities leading to 0. We have used smoothing value to be 0.001 as we do not want to vary the results very much.\n",
    "\n",
    "When we want the algorithm to learn, we use *Odds Ratio*. For every feature and every pair of classes we have, we check each one of them to increase the belief in one class or the other. \n",
    "\n",
    "**Advantages**\n",
    "- It is easy to implement.\n",
    "- Requires just small amount of data to establish the beliefs.\n",
    "- It is less sensitive to lost data.\n",
    "- Speed of training and testing is pretty high\n",
    "\n",
    "**Disadvantages**\n",
    "- We need prior probability.\n",
    "- The assumption that all the features are independent.\n",
    "- If there is a test data that was not seen during training, then the probability of this data would be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    P_A_GIVEN_B = 'P(A|B)'\n",
    "    P_B_GIVEN_A = 'P(B|A)'\n",
    "    P_A = 'P(A)'\n",
    "    P_B = 'P(B)'\n",
    "\n",
    "    # Smotthing\n",
    "    kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "\n",
    "    def __init__(self, FEATURES, LABELS, POSSIBLE_VALUES):\n",
    "        self.LabelMap = {}\n",
    "        self.FeatureMap = {}\n",
    "        self.FEATURES = FEATURES\n",
    "        self.LABELS = LABELS\n",
    "        self.K = 2\n",
    "\n",
    "        # Initialization of FMAP - FEATURES X LABELS X POSSIBLE_VALUES\n",
    "        for featureIndex in range(self.FEATURES):\n",
    "            self.FeatureMap[featureIndex] = {}\n",
    "            for labelIndex in range(self.LABELS):\n",
    "                self.FeatureMap[featureIndex][labelIndex] = {}\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = 0\n",
    "\n",
    "        # Initialization\n",
    "        for labelIndex in range(0, self.LABELS):\n",
    "            self.LabelMap[labelIndex] = 0\n",
    "\n",
    "    def P_A_given_B(self, map):\n",
    "        result = ( map.get(NaiveBayesClassifier.P_B_GIVEN_A) * map.get(NaiveBayesClassifier.P_A) )\\\n",
    "                 / map.get(NaiveBayesClassifier.P_B)\n",
    "        return result\n",
    "\n",
    "    # Constructing Labels probability\n",
    "    # PRIOR DISTRIBUTION OVER LABELS #\n",
    "    def constructLabelsProbability(self, trainingLabels):\n",
    "        totalDataset = len(trainingLabels)\n",
    "\n",
    "        # Storing Frequency\n",
    "        for label in trainingLabels:\n",
    "            self.LabelMap[label] += 1\n",
    "\n",
    "        # Calculating probability -> frequency/total -> LOG\n",
    "        for key in self.LabelMap:\n",
    "            probability = self.LabelMap[key] / totalDataset\n",
    "            self.LabelMap[key] = probability\n",
    "\n",
    "    def constructFeaturesProbability(self, featureValueListForAllTrainingImages, actualLabelForTrainingList, POSSIBLE_VALUES):\n",
    "\n",
    "        # TRAINING\n",
    "        for label, featureValuesPerImage in zip(actualLabelForTrainingList, featureValueListForAllTrainingImages):\n",
    "            for feature in range(0, self.FEATURES):\n",
    "                self.FeatureMap[feature][label][featureValuesPerImage[feature]] += 1\n",
    "\n",
    "        # Converting frequencies to probabilities to it's LOG\n",
    "        for featureIndex in range(self.FEATURES):\n",
    "            for labelIndex in range(self.LABELS):\n",
    "                sum = 0\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    sum += self.FeatureMap.get(featureIndex).get(labelIndex).get(possibleValueIndex) + self.K\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    probability = (self.FeatureMap.get(featureIndex).get(labelIndex).get(possibleValueIndex) + self.K) / sum\n",
    "                    self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = probability\n",
    "\n",
    "    def predictLabel_GivenFeatures(self, featuresListOfImage):\n",
    "        probabilityPerLabel = []\n",
    "        for label in self.LabelMap:\n",
    "            # P(Y=label|features)\n",
    "            P_Y = self.LabelMap.get(label)\n",
    "            P_features_given_Y = 0\n",
    "            for feature in range(0, self.FEATURES):\n",
    "                P_features_given_Y += math.log(self.FeatureMap[feature][label][featuresListOfImage[feature]])\n",
    "            probability = math.log(P_Y, 2) + P_features_given_Y\n",
    "            probabilityPerLabel.append(probability)\n",
    "\n",
    "        predictedLabel = np.argmax(probabilityPerLabel)\n",
    "        return predictedLabel\n",
    "\n",
    "    def testModel(self, featuresListOfImage, actualLabel):\n",
    "        predictedLabel = self.predictLabel_GivenFeatures(featuresListOfImage)\n",
    "        if predictedLabel != actualLabel:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors(KNN)\n",
    "\n",
    "*KNN* algorithm\n",
    "\n",
    "**Advantages**\n",
    "- No training period. Learns when testing the data.\n",
    "- Can accept new data flexibly.\n",
    "- Easy to implement.\n",
    "\n",
    "**Disadvantages**\n",
    "- Accuracy is small with large datasets.\n",
    "- We need to standardize the input to scale them for appropriate predictions.\n",
    "- If some data is missing or the dataset has some error, it can give wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import euclidean_distance, most_frequent\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, num_neighbors):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.trainX = []\n",
    "        self.trainY = []\n",
    "\n",
    "    def test(self, test_row, actualLabel):\n",
    "        neighbors = self.get_neighbors(test_row)\n",
    "        y_pred = most_frequent(neighbors)\n",
    "\n",
    "        if y_pred == actualLabel:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def storeTrainingSet(self, x, y):\n",
    "        self.trainX += x\n",
    "        self.trainY += y\n",
    "\n",
    "    # Locate the most similar neighbors\n",
    "    def get_neighbors(self, test_row):\n",
    "        distances = list()\n",
    "        zippedTrainingData = zip(self.trainX, self.trainY)\n",
    "\n",
    "        for train_row in zippedTrainingData:\n",
    "            features = train_row[0]\n",
    "            label = train_row[1]\n",
    "            dist = euclidean_distance(test_row, features)\n",
    "            distances.append((features, label, dist))\n",
    "\n",
    "        distances.sort(key=lambda tup: tup[2]) # Sort according to the dist\n",
    "        neighbors = list()\n",
    "        for i in range(self.num_neighbors):\n",
    "            neighbors.append(distances[i][1])\n",
    "        return neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a separate class to open the files and iterate through the data in those files. The data from the files are stored in the form of lists of list which is then fed to different functions according to their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Samples:\n",
    "\n",
    "    def __init__(self, DATA_DIR):\n",
    "        self.TestFileObject = None\n",
    "        self.TestLabelFileObject = None\n",
    "        self.TrainFileObject = None\n",
    "        self.TrainLabelFileObject = None\n",
    "        self.ValidationFileObject = None\n",
    "        self.ValidationLabelFileObject = None\n",
    "\n",
    "        self.test_lines_itr = None\n",
    "        self.test_labelsLines_itr = None\n",
    "        self.train_lines_itr = None\n",
    "        self.train_labelsLines_itr = None\n",
    "        self.validate_lines_itr = None\n",
    "        self.validate_labelsLines_itr = None\n",
    "\n",
    "\n",
    "        self.trainingFileName = DATA_DIR + \"/trainingimages\"\n",
    "        self.trainingLabelFileName = DATA_DIR + \"/traininglabels\"\n",
    "        self.testFileName = DATA_DIR + \"/testimages\"\n",
    "        self.testLabelFileName = DATA_DIR + \"/testlabels\"\n",
    "        self.validationFileName = DATA_DIR + \"/validationimages\"\n",
    "        self.validationLabelFileName = DATA_DIR + \"/validationlabels\"\n",
    "\n",
    "        TEST = \"TEST\"\n",
    "        TRAIN = \"TRAIN\"\n",
    "        VALIDATION = \"VALIDATION\"\n",
    "\n",
    "    # def open_many_files(self,file_name):\n",
    "    #     with open(self.name, 'r') as f:\n",
    "    #         c = 0\n",
    "    #         for l in f:\n",
    "    #             c+=1\n",
    "    #         l=c+1-self.k_value\n",
    "    #         for i in range(0,l):\n",
    "    #             lines = [line for line in f][:self.k_value]\n",
    "    #             object=lines\n",
    "    #         return object\n",
    "\n",
    "\n",
    "    def closeFiles(self):\n",
    "        self.TestFileObject.close()\n",
    "        self.TestLabelFileObject.close()\n",
    "        self.TrainFileObject.close()\n",
    "        self.TrainLabelFileObject.close()\n",
    "        self.ValidationFileObject.close()\n",
    "        self.ValidationLabelFileObject.close()\n",
    "\n",
    "    def initTestIters(self):\n",
    "        self.TestFileObject.close()\n",
    "        self.TestLabelFileObject.close()\n",
    "        self.TestFileObject = open(self.testFileName)\n",
    "        self.TestLabelFileObject = open(self.testLabelFileName)\n",
    "        self.test_lines_itr = iter(self.TestFileObject.readlines())\n",
    "        self.test_labelsLines_itr = iter(self.TestLabelFileObject.readlines())\n",
    "\n",
    "    def initValidateIters(self):\n",
    "        self.validate_lines_itr = iter(self.ValidationFileObject.readlines())\n",
    "        self.validate_labelsLines_itr = iter(self.ValidationLabelFileObject.readlines())\n",
    "\n",
    "    def readFiles(self):\n",
    "        self.TrainFileObject = open(self.trainingFileName)\n",
    "        self.TrainLabelFileObject = open(self.trainingLabelFileName)\n",
    "        self.TestFileObject = open(self.testFileName)\n",
    "        self.TestLabelFileObject = open(self.testLabelFileName)\n",
    "        self.ValidationFileObject = open(self.validationFileName)\n",
    "        self.ValidationLabelFileObject = open(self.validationLabelFileName)\n",
    "\n",
    "        self.train_lines_itr = iter(self.TrainFileObject.readlines())\n",
    "        self.train_labelsLines_itr = iter(self.TrainLabelFileObject.readlines())\n",
    "\n",
    "        self.test_lines_itr = iter(self.TestFileObject.readlines())\n",
    "        self.test_labelsLines_itr = iter(self.TestLabelFileObject.readlines())\n",
    "\n",
    "        self.initValidateIters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to plot the error data in the form of graphs.\n",
    "- First value is the dataset that we would be plotting on the $x$ axis. \n",
    "- Second value is the list of error rates for all the three algorithms. \n",
    "- The type of the dataset (\"FACE\" or \"DIGIT\") is passed here for the title of the graph.\n",
    "- And the final parameter is the name of the algorithm so that we can specify which curve is for which graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "class Error:\n",
    "\n",
    "    def graphplot(self, dataset, errorRateList, type, method):\n",
    "        for i in range(len(errorRateList)):\n",
    "            plt.plot(dataset, errorRateList[i], label=type[i])\n",
    "            plt.xlim(0, dataset[-1] + dataset[-1]/10)\n",
    "            plt.ylim(0, 100)\n",
    "\n",
    "        for i in range(len(errorRateList)):\n",
    "            for data, errorRate in zip(dataset, errorRateList[i]):\n",
    "                pyplot.text(data, errorRate, str(int(errorRate)))\n",
    "\n",
    "        plt.title(method)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function of the code. All other classes are imported in this file and are accessed here as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING OUR MODEL FIRST\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type FACE or DIGIT FACE\n",
      "Value of Grid 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 0 to 75 data\n",
      "TESTING our model that is TRAINED ON 0 to 75 data\n",
      "Error is 51 out of Total of  149  :  34.22818791946309\n",
      "Error is 52 out of Total of  149  :  34.899328859060404\n",
      "Error is 58 out of Total of  149  :  38.92617449664429\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 75 to 150 data\n",
      "TESTING our model that is TRAINED ON 0 to 150 data\n",
      "Error is 40 out of Total of  149  :  26.845637583892618\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Error is 47 out of Total of  149  :  31.543624161073826\n",
      "Perceptron  mean =  30.536912751677853  and Standard Deviation =  5.2202514047329025\n",
      "Bayes  mean =  24.496644295302012  and Standard Deviation =  14.711617595156358\n",
      "KNN  mean =  35.23489932885906  and Standard Deviation =  5.2202514047329\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 150 to 225 data\n",
      "TESTING our model that is TRAINED ON 0 to 225 data\n",
      "Error is 34 out of Total of  149  :  22.818791946308725\n",
      "Error is 26 out of Total of  149  :  17.449664429530202\n",
      "Error is 38 out of Total of  149  :  25.503355704697988\n",
      "Perceptron  mean =  27.96420581655481  and Standard Deviation =  5.786361143793094\n",
      "Bayes  mean =  22.14765100671141  and Standard Deviation =  11.170011394022309\n",
      "KNN  mean =  31.991051454138702  and Standard Deviation =  6.722585772401215\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 225 to 300 data\n",
      "TESTING our model that is TRAINED ON 0 to 300 data\n",
      "Error is 35 out of Total of  149  :  23.48993288590604\n",
      "Error is 32 out of Total of  149  :  21.476510067114095\n",
      "Error is 32 out of Total of  149  :  21.476510067114095\n",
      "Perceptron  mean =  26.845637583892618  and Standard Deviation =  5.2274368883883335\n",
      "Bayes  mean =  21.97986577181208  and Standard Deviation =  9.126447496256489\n",
      "KNN  mean =  29.36241610738255  and Standard Deviation =  7.600504465037666\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 300 to 375 data\n",
      "TESTING our model that is TRAINED ON 0 to 375 data\n",
      "Error is 36 out of Total of  149  :  24.161073825503355\n",
      "Error is 27 out of Total of  149  :  18.120805369127517\n",
      "Error is 31 out of Total of  149  :  20.80536912751678\n",
      "Perceptron  mean =  26.308724832214764  and Standard Deviation =  4.683582905333666\n",
      "Bayes  mean =  21.20805369127517  and Standard Deviation =  8.089963063250558\n",
      "KNN  mean =  27.651006711409398  and Standard Deviation =  7.613827007190214\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 375 to 450 data\n",
      "TESTING our model that is TRAINED ON 0 to 450 data\n",
      "Error is 40 out of Total of  149  :  26.845637583892618\n",
      "Error is 39 out of Total of  149  :  26.174496644295303\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Perceptron  mean =  26.39821029082774  and Standard Deviation =  4.1948545801643755\n",
      "Bayes  mean =  22.03579418344519  and Standard Deviation =  7.514581014672336\n",
      "KNN  mean =  25.39149888143177  and Standard Deviation =  8.775451165536538\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 450 to 525 data\n",
      "TESTING our model that is TRAINED ON 0 to 525 data\n",
      "Error is 30 out of Total of  149  :  20.13422818791946\n",
      "Error is 23 out of Total of  149  :  15.436241610738255\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Perceptron  mean =  25.503355704697988  and Standard Deviation =  4.502150290268034\n",
      "Bayes  mean =  21.09300095877277  and Standard Deviation =  7.299277587986301\n",
      "KNN  mean =  23.777564717162033  and Standard Deviation =  9.077845029068103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 525 to 600 data\n",
      "TESTING our model that is TRAINED ON 0 to 600 data\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Error is 24 out of Total of  149  :  16.107382550335572\n",
      "Error is 38 out of Total of  149  :  25.503355704697988\n",
      "Perceptron  mean =  24.07718120805369  and Standard Deviation =  5.800476216043021\n",
      "Bayes  mean =  20.469798657718123  and Standard Deviation =  6.9839209401627285\n",
      "KNN  mean =  23.993288590604028  and Standard Deviation =  8.426570921341073\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 600 to 675 data\n",
      "TESTING our model that is TRAINED ON 0 to 675 data\n",
      "Error is 22 out of Total of  149  :  14.765100671140939\n",
      "Error is 24 out of Total of  149  :  16.107382550335572\n",
      "Error is 38 out of Total of  149  :  25.503355704697988\n",
      "Perceptron  mean =  23.042505592841163  and Standard Deviation =  6.250985238321195\n",
      "Bayes  mean =  19.98508575689784  and Standard Deviation =  6.692740627137177\n",
      "KNN  mean =  24.161073825503355  and Standard Deviation =  7.898390802644174\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 675 to 750 data\n",
      "TESTING our model that is TRAINED ON 0 to 750 data\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Error is 21 out of Total of  149  :  14.093959731543624\n",
      "Error is 30 out of Total of  149  :  20.13422818791946\n",
      "Perceptron  mean =  22.14765100671141  and Standard Deviation =  6.537646172797521\n",
      "Bayes  mean =  19.395973154362416  and Standard Deviation =  6.57923540035032\n",
      "KNN  mean =  23.758389261744966  and Standard Deviation =  7.5547671144368485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Training ON 750 to 825 data\n",
      "TESTING our model that is TRAINED ON 0 to 825 data\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from knn import KNN\n",
    "import matplotlib.pyplot as plt\n",
    "from naivebyes import NaiveBayesClassifier\n",
    "from perceptron import PerceptronClassifier\n",
    "from error_plot import Error\n",
    "from samples import Samples\n",
    "\n",
    "\n",
    "def mean_standard_deviation(errorRate, name):\n",
    "    if len(errorRate) > 1:\n",
    "        mean = statistics.mean(errorRate)\n",
    "        standard_deviation = statistics.stdev(errorRate)\n",
    "        print(name, \" mean = \", mean, \" and Standard Deviation = \", standard_deviation)\n",
    "        return mean\n",
    "\n",
    "\n",
    "class DataClassifier:\n",
    "    def __init__(self, imgHeight, imgWidth, LABELS, pixelChars, pixelGrid):\n",
    "        if pixelChars is None:\n",
    "            pixelChars = ['#', '+']\n",
    "        self.pixelGrid = pixelGrid\n",
    "        self.imgHeight = imgHeight\n",
    "        self.imgWidth = imgWidth\n",
    "        self.FEATURES = math.ceil((imgHeight - self.pixelGrid + 1) * (imgWidth - self.pixelGrid + 1))\n",
    "        self.LABELS = LABELS\n",
    "        self.pixelChars = pixelChars\n",
    "        self.FileObject = None\n",
    "        self.LabelFileObject = None\n",
    "\n",
    "    def countPixels(self, line):\n",
    "        count = 0\n",
    "        if not isinstance(line, list):\n",
    "            line = list(line)\n",
    "\n",
    "        for char in line:\n",
    "            if char in self.pixelChars:\n",
    "                count += 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    def extractFeaturesPerLine(self, line, row):\n",
    "        gridList = []\n",
    "        featureValueList = []\n",
    "\n",
    "        for startIndexOfGrid in range(0, len(line), 1):\n",
    "            gridList.append(line[startIndexOfGrid:startIndexOfGrid + 1])\n",
    "\n",
    "        # col = 0\n",
    "        for grid in gridList:\n",
    "            # Count the number of chars in this grid and add the count to respective index of FEATURE\n",
    "            # indexOfFeature = row + col\n",
    "            featureValueList.append(self.countPixels(grid))\n",
    "\n",
    "        return featureValueList\n",
    "\n",
    "    def splitImageLineFeaturesIntoGridFeatures(self, imageLinesList, gridSize):\n",
    "        height_rows = self.imgHeight + 1 - gridSize\n",
    "        width_rows = self.imgWidth + 1 - gridSize\n",
    "        height_new_list = []\n",
    "\n",
    "        for rowIndex in range(0, self.imgHeight):\n",
    "            line = imageLinesList[rowIndex]\n",
    "            width_new_list = []\n",
    "            for gridStartIndex in range(0, width_rows):\n",
    "                width_new_list.append(sum(line[gridStartIndex: gridStartIndex + gridSize]))\n",
    "            height_new_list.append(width_new_list)\n",
    "\n",
    "        featureListForImage = []\n",
    "        for rowIndex in range(0, height_rows):\n",
    "            for column in range(0, width_rows):\n",
    "                sum1 = 0\n",
    "                for rows in range(0, gridSize):\n",
    "                    sum1 += height_new_list[rowIndex + rows][column]\n",
    "                featureListForImage.append(sum1)\n",
    "\n",
    "        return featureListForImage\n",
    "\n",
    "    def extractFeatures(self, lines_itr, labelsLines_itr):\n",
    "        imageLine = lines_itr.__next__()\n",
    "\n",
    "        totalImages = 0\n",
    "        featureValueListForAllTestingImages = []\n",
    "        actualLabelList = []\n",
    "\n",
    "        try:\n",
    "            while imageLine:\n",
    "                # Skipping the blank lines\n",
    "                while imageLine and self.countPixels(imageLine) == 0:\n",
    "                    imageLine = lines_itr.__next__()\n",
    "\n",
    "                imageLinesList = []\n",
    "                # Scanning image pixels\n",
    "                for i in range(0, self.imgHeight):\n",
    "                    imageLinesList.append(self.extractFeaturesPerLine(imageLine, i))\n",
    "                    # print(featureValueList)\n",
    "                    imageLine = lines_itr.__next__()\n",
    "\n",
    "                featureValueListPerImage = self.splitImageLineFeaturesIntoGridFeatures(imageLinesList, gridSize)\n",
    "\n",
    "                totalImages += 1\n",
    "                actualLabel = labelsLines_itr.__next__()\n",
    "\n",
    "                featureValueListForAllTestingImages.append(featureValueListPerImage)\n",
    "                actualLabelList.append(int(actualLabel))\n",
    "\n",
    "        except StopIteration:\n",
    "            # print(\"End of File\")\n",
    "            pass\n",
    "\n",
    "        return featureValueListForAllTestingImages, actualLabelList\n",
    "\n",
    "\n",
    "def error(errorPrediction, total):\n",
    "    errorRate = (errorPrediction * 100) / total\n",
    "    print(\"Error is\", errorPrediction, \"out of Total of \", total, \" : \", errorRate)\n",
    "    return errorRate\n",
    "\n",
    "\n",
    "FACE = \"FACE\"\n",
    "DIGIT = \"DIGIT\"\n",
    "DIR = \"DIR\"\n",
    "HEIGHT = \"HEIGHT\"\n",
    "WIDTH = \"WIDTH\"\n",
    "LABEL = \"LABEL\"\n",
    "PIXELS = \"PIXELS\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"TRAINING OUR MODEL FIRST\")\n",
    "    PERCENT_INCREMENT = 10\n",
    "\n",
    "    perceptron_y = []\n",
    "    bayes_y = []\n",
    "    knn_y = []\n",
    "    dataSetIncrements = []\n",
    "    perceptron_time = []\n",
    "    bayes_time = []\n",
    "    knn_time = []\n",
    "    perceptron_msd=[]\n",
    "    bayes_msd=[]\n",
    "    knn_msd=[]\n",
    "\n",
    "    inp = input(\"Type FACE or DIGIT\")\n",
    "    gridSize = int(input(\"Value of Grid\"))\n",
    "    POSSIBLE_VALUES = [x for x in range(0, gridSize * gridSize + 1)]\n",
    "\n",
    "    map = {\n",
    "        FACE: {\n",
    "            DIR: 'data/facedata', HEIGHT: 68, WIDTH: 61, LABEL: 2, PIXELS: None\n",
    "        },\n",
    "        DIGIT: {\n",
    "            DIR: 'data/digitdata', HEIGHT: 20, WIDTH: 29, LABEL: 10, PIXELS: None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    dataType = map.get(inp)\n",
    "    samples = Samples(dataType.get(DIR))\n",
    "\n",
    "    dataClassifier = DataClassifier(dataType.get(HEIGHT), dataType.get(WIDTH), dataType.get(LABEL),\n",
    "                                    dataType.get(PIXELS), gridSize)\n",
    "    perceptronClassifier = PerceptronClassifier(dataClassifier.FEATURES, dataClassifier.LABELS)\n",
    "\n",
    "    samples.readFiles()\n",
    "\n",
    "    # Extracting Features from the Training Data\n",
    "    dataset = 0\n",
    "    featureValueListForAllTrainingImages, actualLabelForTrainingList = \\\n",
    "        dataClassifier.extractFeatures(samples.train_lines_itr, samples.train_labelsLines_itr)\n",
    "\n",
    "    TOTALDATASET = len(actualLabelForTrainingList)\n",
    "    INCREMENTS = int(TOTALDATASET * PERCENT_INCREMENT / 100)\n",
    "    PERCEPTRON_TIME = {}\n",
    "\n",
    "    # Initialization of Classifiers\n",
    "    perceptronClassifier = PerceptronClassifier(dataClassifier.FEATURES, dataClassifier.LABELS)\n",
    "    naiveBayesClassifier = NaiveBayesClassifier(dataClassifier.FEATURES, dataClassifier.LABELS, POSSIBLE_VALUES)\n",
    "    KNNClassifier = KNN(num_neighbors=20)\n",
    "\n",
    "    featureValueListForAllTestingImages = actualTestingLabelList = []\n",
    "    while dataset < TOTALDATASET:\n",
    "\n",
    "        featureValueList_currentTrainingImages = featureValueListForAllTrainingImages[dataset:dataset + INCREMENTS]\n",
    "        actualLabel_currentTrainingImages = actualLabelForTrainingList[dataset:dataset + INCREMENTS]\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\\n Training ON {0} to {1} data\".format(dataset, dataset + INCREMENTS))\n",
    "        ImageFeatureLabelZipList = zip(featureValueList_currentTrainingImages, actualLabel_currentTrainingImages)\n",
    "\n",
    "        startTimer = time.time()\n",
    "        ''' ####################  TRAINING PHASE FOR PERCEPTRON ############# '''\n",
    "        for featureValueListPerImage, actualLabel in ImageFeatureLabelZipList:\n",
    "            perceptronClassifier.runModel(True, featureValueListPerImage, actualLabel)\n",
    "        endTimer = time.time()\n",
    "\n",
    "        perceptron_time.append(endTimer - startTimer)\n",
    "\n",
    "        startTimer = time.time()\n",
    "        ''' ####################  TRAINING PHASE FOR NAIVE BYES ############# '''\n",
    "        naiveBayesClassifier.constructLabelsProbability(actualLabel_currentTrainingImages)\n",
    "        naiveBayesClassifier.constructFeaturesProbability(featureValueList_currentTrainingImages,\n",
    "                                                          actualLabel_currentTrainingImages,\n",
    "                                                          POSSIBLE_VALUES)\n",
    "        endTimer = time.time()\n",
    "\n",
    "        bayes_time.append(endTimer - startTimer)\n",
    "\n",
    "        ''' ################## NO TRAINING PHASE FOR KNN #################  '''\n",
    "        KNNClassifier.storeTrainingSet(featureValueList_currentTrainingImages, actualLabel_currentTrainingImages)\n",
    "        ''' SIMPLY STORING FOR KNN '''\n",
    "\n",
    "        ''' ####################  TESTING PHASE ############# '''\n",
    "        samples.initTestIters()\n",
    "\n",
    "        print(\"TESTING our model that is TRAINED ON {0} to {1} data\".format(0, dataset + INCREMENTS))\n",
    "\n",
    "        perceptron_errorPrediction = naiveByes_errorPrediction = knn_errorPrediction = total = 0\n",
    "        featureValueListForAllTestingImages, actualTestingLabelList = \\\n",
    "            dataClassifier.extractFeatures(samples.test_lines_itr, samples.test_labelsLines_itr)\n",
    "\n",
    "        for featureValueListPerImage, actualLabel in zip(featureValueListForAllTestingImages, actualTestingLabelList):\n",
    "            perceptron_errorPrediction += perceptronClassifier.runModel(False, featureValueListPerImage, actualLabel)\n",
    "            naiveByes_errorPrediction += naiveBayesClassifier.testModel(featureValueListPerImage, actualLabel)\n",
    "\n",
    "            ''' ####################  TESTING PHASE FOR KNN ############# '''\n",
    "            startTimer = time.time()\n",
    "\n",
    "            knn_errorPrediction += KNNClassifier.test(featureValueListPerImage, actualLabel)\n",
    "\n",
    "            endTimer = time.time()\n",
    "            knn_time.append(endTimer - startTimer)\n",
    "            ''' ####################  TESTING PHASE OVER FOR KNN ############# '''\n",
    "\n",
    "            total += 1\n",
    "\n",
    "        perceptron_error = error(perceptron_errorPrediction, total)\n",
    "        bayes_error = error(naiveByes_errorPrediction, total)\n",
    "        knn_error = error(knn_errorPrediction, total)\n",
    "        perceptron_msd.append(perceptron_error)\n",
    "        bayes_msd.append(bayes_error)\n",
    "        knn_msd.append(knn_error)\n",
    "\n",
    "        perceptron_msd_graph = mean_standard_deviation(perceptron_msd,\"Perceptron\")\n",
    "        bayes_msd_graph = mean_standard_deviation(bayes_msd,\"Bayes\")\n",
    "        knn_msd_graph = mean_standard_deviation(knn_msd,\"KNN\")\n",
    "\n",
    "        dataset += INCREMENTS\n",
    "\n",
    "        dataSetIncrements.append(dataset)\n",
    "        perceptron_y.append(perceptron_error)\n",
    "        bayes_y.append(bayes_error)\n",
    "        knn_y.append(knn_error)\n",
    "\n",
    "    final_array = {\n",
    "        1: [perceptron_y, bayes_y, knn_y],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    final_array2 = {\n",
    "        1: [perceptron_time, bayes_time, knn_time],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    final_array3 = {\n",
    "        1: [perceptron_msd_graph, bayes_msd_graph, knn_msd_graph],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    error = Error()\n",
    "    error.graphplot(dataSetIncrements, final_array.get(1), final_array.get(2), inp) #For error plotting\n",
    "    # error.graphplot(dataSetIncrements, final_array2.get(1), final_array2.get(2), inp) #For time\n",
    "#     error.graphplot(dataSetIncrements, final_array3.get(1), final_array3.get(2), inp) #For mean\n",
    "    # error.graphplot(dataSetIncrements, final_array3.get(1)[1], final_array3.get(2), inp) #For Standard Deviation\n",
    "\n",
    "    samples.closeFiles()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
