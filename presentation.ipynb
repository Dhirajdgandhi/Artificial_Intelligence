{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Single Pixel\n",
    "\n",
    "Our Input Data contains ['#','+'] as pixel values and we consider a weight of 1 for each of these. Having different values for these characters gives the same result. We transform our input data into binary values depending on the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelChars = ['#', '+']\n",
    "\n",
    "def countPixels(self, line):\n",
    "    count = 0\n",
    "\n",
    "    if char in self.pixelChars:\n",
    "        count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Grid of Pixels\n",
    "\n",
    "We have considered successive grids of size n*\\n as our features to give a better description of input to our classifier. This gave us amazing result improvements increasing accuracy over 10% for every classifier. We calculate total sum of pixels which are valued 1, for a GRID of size n\\*n and store this as our feature.   \n",
    "\n",
    "We observed the following error rate for varied grid sizes for our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: classifier per grid size results (graph maybe)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We have **FACE** and **DIGIT** datasets. We have combined the Training and Validation Datasets for our Training. The Testing dataset is purely used for Testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Algorithms \n",
    "\n",
    "## 1. Perceptron \n",
    "\n",
    "*Perceptron* is a supervised learning algorithm where we train our algorithm from a *labeled* training dataset.\n",
    "\n",
    "Weight matrix is of SHAPE as below and we initialize our weights with random numbers. Every LABEL has a weight array of FEATURES + 1 length. The + 1 is for the bias weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, FEATURES, LABELS):\n",
    "    self.SHAPE = (LABELS, FEATURES + 1)  # The +1 is for our w0 weight.\n",
    "    self.weightMatrix = np.zeros(self.SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training :\n",
    "\n",
    "For Traning, we calculate the dot product between the weights and feature values and sum up these. We do this for all the labels and find the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(self, isTrain, featureValueList, actualLabel):\n",
    "    for labelWeights in self.weightMatrix:\n",
    "        featureScoreList.append(np.sum(np.dot(labelWeights, featureValueList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum probability is our predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bac936a0c8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictedLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureScoreList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "predictedLabel = np.argmax(featureScoreList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight updation :\n",
    "If the predicted and actual lables do not match, we update the weights by adding/subtracting the feature values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictedLabel != actualLabel:\n",
    "    if isTrain:\n",
    "        self.updateWeights(predictedLabel, actualLabel, featureValueList)\n",
    "    else: return 1\n",
    "else: return 0\n",
    "    \n",
    "def updateWeights(self, predictedLabel, actualLabel, featureValueList):\n",
    "    self.weightMatrix[actualLabel] = self.weightMatrix[actualLabel] + featureValueList\n",
    "    self.weightMatrix[predictedLabel] = self.weightMatrix[predictedLabel] - featureValueList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Command to run this classifier with arguments. ANd show their graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "- Perceptrons have the ability to learn themselves the complex relationships and patterns in the dataset.\n",
    "- We can have any type of input. It does not restrict to use any one datatype as inputs.\n",
    "- If we have a single layer of perceptron, then the training is very quick.\n",
    "- Is really accurate for image processing and character recognition.\n",
    "\n",
    "### **Disadvantages**\n",
    "- A single layer of perceptron cannot train a problem whose solution is a non-linear function.\n",
    "- Multi layer perceptron takes more time to train.\n",
    "- Difficult optimization if we have a lot of local minima/maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Naive Bayes Classifier\n",
    "*Naive Bayes Classifier* is a part of the probabilistic classifiers based on Bayes' Theorem. The formula for Bayes' theorem is $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$. There are two assumptions taken in this algorithm.\n",
    "- That the feature is independent.\n",
    "- The importance of every feature is equal.\n",
    "\n",
    "### Initialization\n",
    "2 maps to store our frequency count probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of FMAP - FEATURES X LABELS X POSSIBLE_VALUES\n",
    "for featureIndex in range(self, self.FEATURES):\n",
    "    for labelIndex in range(self.LABELS):\n",
    "        for possibleValueIndex in POSSIBLE_VALUES:\n",
    "            self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = 0\n",
    "\n",
    "# Initialization of LMAP - LABELS\n",
    "for labelIndex in range(self, 0, self.LABELS):\n",
    "    self.LabelMap[labelIndex] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We fill the 2 maps initialized, with our input data features frequency and then calculate the LOG(Probabilities(frequency)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructLabelsProbability(self, trainingLabels):\n",
    "    # Storing Frequency\n",
    "    for label in trainingLabels: self.LabelMap[label] += 1\n",
    "\n",
    "    # Calculating probability -> frequency/total -> LOG\n",
    "    for key in self.LabelMap:\n",
    "        probability = self.LabelMap[key] / totalDataset\n",
    "        self.LabelMap[key] = probability\n",
    "\n",
    "def constructFeaturesProbability(self, featureValueListForAllTrainingImages, actualLabelForTrainingList, POSSIBLE_VALUES):\n",
    "\n",
    "    # TRAINING\n",
    "    for label, featureValuesPerImage in zip(actualLabelForTrainingList, featureValueListForAllTrainingImages):\n",
    "        for feature in range(0, self.FEATURES):\n",
    "            self.FeatureMap[feature][label][featureValuesPerImage[feature]] += 1\n",
    "\n",
    "    # Then Converting frequencies to probabilities and then to it's LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "In real life, we do not want to set the probabilities of any term to be 0. But, if the algorithm does not see any connections between $A$ and $B$ in the formula, it would give $P(A|B) = 0$. This is not acceptable in real world predications and hence, we have used *Laplace Smoothing* to get rid of any probabilities leading to 0.\n",
    "\n",
    "We experimented with the smoothing value with these values and found the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "\n",
    "# print graphs with the above results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odds Ratio\n",
    "When we want the algorithm to learn, we use *Odds Ratio*. For every feature and every pair of classes we have, we check each one of them to increase the belief in one class or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print # TODO: Add all the observation graph here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- It is easy to implement.\n",
    "- Requires just small amount of data to establish the beliefs.\n",
    "- It is less sensitive to lost data.\n",
    "- Speed of training and testing is pretty high\n",
    "\n",
    "**Disadvantages**\n",
    "- We need prior probability.\n",
    "- The assumption that all the features are independent.\n",
    "- If there is a test data that was not seen during training, then the probability of this data would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. K-Nearest Neighbors(KNN)\n",
    "\n",
    "*KNN* algorithm\n",
    "\n",
    "**Advantages**\n",
    "- No training period. Learns when testing the data.\n",
    "- Can accept new data flexibly.\n",
    "- Easy to implement.\n",
    "\n",
    "**Disadvantages**\n",
    "- Accuracy is small with large datasets.\n",
    "- We need to standardize the input to scale them for appropriate predictions.\n",
    "- If some data is missing or the dataset has some error, it can give wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import euclidean_distance, most_frequent\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, num_neighbors):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.trainX = []\n",
    "        self.trainY = []\n",
    "\n",
    "    def test(self, test_row, actualLabel):\n",
    "        neighbors = self.get_neighbors(test_row)\n",
    "        y_pred = most_frequent(neighbors)\n",
    "\n",
    "        if y_pred == actualLabel:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def storeTrainingSet(self, x, y):\n",
    "        self.trainX += x\n",
    "        self.trainY += y\n",
    "\n",
    "    # Locate the most similar neighbors\n",
    "    def get_neighbors(self, test_row):\n",
    "        distances = list()\n",
    "        zippedTrainingData = zip(self.trainX, self.trainY)\n",
    "\n",
    "        for train_row in zippedTrainingData:\n",
    "            features = train_row[0]\n",
    "            label = train_row[1]\n",
    "            dist = euclidean_distance(test_row, features)\n",
    "            distances.append((features, label, dist))\n",
    "\n",
    "        distances.sort(key=lambda tup: tup[2]) # Sort according to the dist\n",
    "        neighbors = list()\n",
    "        for i in range(self.num_neighbors):\n",
    "            neighbors.append(distances[i][1])\n",
    "        return neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have created a separate class to open the files and iterate through the data in those files. The data from the files are stored in the form of lists of list which is then fed to different functions according to their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Samples:\n",
    "\n",
    "    def __init__(self, DATA_DIR):\n",
    "        self.TestFileObject = None\n",
    "        self.TestLabelFileObject = None\n",
    "        self.TrainFileObject = None\n",
    "        self.TrainLabelFileObject = None\n",
    "        self.ValidationFileObject = None\n",
    "        self.ValidationLabelFileObject = None\n",
    "\n",
    "        self.test_lines_itr = None\n",
    "        self.test_labelsLines_itr = None\n",
    "        self.train_lines_itr = None\n",
    "        self.train_labelsLines_itr = None\n",
    "        self.validate_lines_itr = None\n",
    "        self.validate_labelsLines_itr = None\n",
    "\n",
    "\n",
    "        self.trainingFileName = DATA_DIR + \"/trainingimages\"\n",
    "        self.trainingLabelFileName = DATA_DIR + \"/traininglabels\"\n",
    "        self.testFileName = DATA_DIR + \"/testimages\"\n",
    "        self.testLabelFileName = DATA_DIR + \"/testlabels\"\n",
    "        self.validationFileName = DATA_DIR + \"/validationimages\"\n",
    "        self.validationLabelFileName = DATA_DIR + \"/validationlabels\"\n",
    "\n",
    "        TEST = \"TEST\"\n",
    "        TRAIN = \"TRAIN\"\n",
    "        VALIDATION = \"VALIDATION\"\n",
    "\n",
    "    # def open_many_files(self,file_name):\n",
    "    #     with open(self.name, 'r') as f:\n",
    "    #         c = 0\n",
    "    #         for l in f:\n",
    "    #             c+=1\n",
    "    #         l=c+1-self.k_value\n",
    "    #         for i in range(0,l):\n",
    "    #             lines = [line for line in f][:self.k_value]\n",
    "    #             object=lines\n",
    "    #         return object\n",
    "\n",
    "\n",
    "    def closeFiles(self):\n",
    "        self.TestFileObject.close()\n",
    "        self.TestLabelFileObject.close()\n",
    "        self.TrainFileObject.close()\n",
    "        self.TrainLabelFileObject.close()\n",
    "        self.ValidationFileObject.close()\n",
    "        self.ValidationLabelFileObject.close()\n",
    "\n",
    "    def initTestIters(self):\n",
    "        self.TestFileObject.close()\n",
    "        self.TestLabelFileObject.close()\n",
    "        self.TestFileObject = open(self.testFileName)\n",
    "        self.TestLabelFileObject = open(self.testLabelFileName)\n",
    "        self.test_lines_itr = iter(self.TestFileObject.readlines())\n",
    "        self.test_labelsLines_itr = iter(self.TestLabelFileObject.readlines())\n",
    "\n",
    "    def initValidateIters(self):\n",
    "        self.validate_lines_itr = iter(self.ValidationFileObject.readlines())\n",
    "        self.validate_labelsLines_itr = iter(self.ValidationLabelFileObject.readlines())\n",
    "\n",
    "    def readFiles(self):\n",
    "        self.TrainFileObject = open(self.trainingFileName)\n",
    "        self.TrainLabelFileObject = open(self.trainingLabelFileName)\n",
    "        self.TestFileObject = open(self.testFileName)\n",
    "        self.TestLabelFileObject = open(self.testLabelFileName)\n",
    "        self.ValidationFileObject = open(self.validationFileName)\n",
    "        self.ValidationLabelFileObject = open(self.validationLabelFileName)\n",
    "\n",
    "        self.train_lines_itr = iter(self.TrainFileObject.readlines())\n",
    "        self.train_labelsLines_itr = iter(self.TrainLabelFileObject.readlines())\n",
    "\n",
    "        self.test_lines_itr = iter(self.TestFileObject.readlines())\n",
    "        self.test_labelsLines_itr = iter(self.TestLabelFileObject.readlines())\n",
    "\n",
    "        self.initValidateIters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to plot the error data in the form of graphs.\n",
    "- First value is the dataset that we would be plotting on the $x$ axis. \n",
    "- Second value is the list of error rates for all the three algorithms. \n",
    "- The type of the dataset (\"FACE\" or \"DIGIT\") is passed here for the title of the graph.\n",
    "- And the final parameter is the name of the algorithm so that we can specify which curve is for which graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "class Error:\n",
    "\n",
    "    def graphplot(self, dataset, errorRateList, type, method):\n",
    "        for i in range(len(errorRateList)):\n",
    "            plt.plot(dataset, errorRateList[i], label=type[i])\n",
    "            plt.xlim(0, dataset[-1] + dataset[-1]/10)\n",
    "            plt.ylim(0, 100)\n",
    "\n",
    "        for i in range(len(errorRateList)):\n",
    "            for data, errorRate in zip(dataset, errorRateList[i]):\n",
    "                pyplot.text(data, errorRate, str(int(errorRate)))\n",
    "\n",
    "        plt.title(method)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function of the code. All other classes are imported in this file and are accessed here as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from knn import KNN\n",
    "import matplotlib.pyplot as plt\n",
    "from naivebyes import NaiveBayesClassifier\n",
    "from perceptron import PerceptronClassifier\n",
    "from error_plot import Error\n",
    "from samples import Samples\n",
    "\n",
    "\n",
    "def mean_standard_deviation(errorRate, name):\n",
    "    if len(errorRate) > 1:\n",
    "        mean = statistics.mean(errorRate)\n",
    "        standard_deviation = statistics.stdev(errorRate)\n",
    "        print(name, \" mean = \", mean, \" and Standard Deviation = \", standard_deviation)\n",
    "        return mean\n",
    "\n",
    "\n",
    "class DataClassifier:\n",
    "    def __init__(self, imgHeight, imgWidth, LABELS, pixelChars, pixelGrid):\n",
    "        if pixelChars is None:\n",
    "            pixelChars = ['#', '+']\n",
    "        self.pixelGrid = pixelGrid\n",
    "        self.imgHeight = imgHeight\n",
    "        self.imgWidth = imgWidth\n",
    "        self.FEATURES = math.ceil((imgHeight - self.pixelGrid + 1) * (imgWidth - self.pixelGrid + 1))\n",
    "        self.LABELS = LABELS\n",
    "        self.pixelChars = pixelChars\n",
    "        self.FileObject = None\n",
    "        self.LabelFileObject = None\n",
    "\n",
    "    def countPixels(self, line):\n",
    "        count = 0\n",
    "        if not isinstance(line, list):\n",
    "            line = list(line)\n",
    "\n",
    "        for char in line:\n",
    "            if char in self.pixelChars:\n",
    "                count += 1\n",
    "\n",
    "        return count\n",
    "\n",
    "    def extractFeaturesPerLine(self, line, row):\n",
    "        gridList = []\n",
    "        featureValueList = []\n",
    "\n",
    "        for startIndexOfGrid in range(0, len(line), 1):\n",
    "            gridList.append(line[startIndexOfGrid:startIndexOfGrid + 1])\n",
    "\n",
    "        # col = 0\n",
    "        for grid in gridList:\n",
    "            # Count the number of chars in this grid and add the count to respective index of FEATURE\n",
    "            # indexOfFeature = row + col\n",
    "            featureValueList.append(self.countPixels(grid))\n",
    "\n",
    "        return featureValueList\n",
    "\n",
    "    def splitImageLineFeaturesIntoGridFeatures(self, imageLinesList, gridSize):\n",
    "        height_rows = self.imgHeight + 1 - gridSize\n",
    "        width_rows = self.imgWidth + 1 - gridSize\n",
    "        height_new_list = []\n",
    "\n",
    "        for rowIndex in range(0, self.imgHeight):\n",
    "            line = imageLinesList[rowIndex]\n",
    "            width_new_list = []\n",
    "            for gridStartIndex in range(0, width_rows):\n",
    "                width_new_list.append(sum(line[gridStartIndex: gridStartIndex + gridSize]))\n",
    "            height_new_list.append(width_new_list)\n",
    "\n",
    "        featureListForImage = []\n",
    "        for rowIndex in range(0, height_rows):\n",
    "            for column in range(0, width_rows):\n",
    "                sum1 = 0\n",
    "                for rows in range(0, gridSize):\n",
    "                    sum1 += height_new_list[rowIndex + rows][column]\n",
    "                featureListForImage.append(sum1)\n",
    "\n",
    "        return featureListForImage\n",
    "\n",
    "    def extractFeatures(self, lines_itr, labelsLines_itr):\n",
    "        imageLine = lines_itr.__next__()\n",
    "\n",
    "        totalImages = 0\n",
    "        featureValueListForAllTestingImages = []\n",
    "        actualLabelList = []\n",
    "\n",
    "        try:\n",
    "            while imageLine:\n",
    "                # Skipping the blank lines\n",
    "                while imageLine and self.countPixels(imageLine) == 0:\n",
    "                    imageLine = lines_itr.__next__()\n",
    "\n",
    "                imageLinesList = []\n",
    "                # Scanning image pixels\n",
    "                for i in range(0, self.imgHeight):\n",
    "                    imageLinesList.append(self.extractFeaturesPerLine(imageLine, i))\n",
    "                    # print(featureValueList)\n",
    "                    imageLine = lines_itr.__next__()\n",
    "\n",
    "                featureValueListPerImage = self.splitImageLineFeaturesIntoGridFeatures(imageLinesList, gridSize)\n",
    "\n",
    "                totalImages += 1\n",
    "                actualLabel = labelsLines_itr.__next__()\n",
    "\n",
    "                featureValueListForAllTestingImages.append(featureValueListPerImage)\n",
    "                actualLabelList.append(int(actualLabel))\n",
    "\n",
    "        except StopIteration:\n",
    "            # print(\"End of File\")\n",
    "            pass\n",
    "\n",
    "        return featureValueListForAllTestingImages, actualLabelList\n",
    "\n",
    "\n",
    "def error(errorPrediction, total):\n",
    "    errorRate = (errorPrediction * 100) / total\n",
    "    print(\"Error is\", errorPrediction, \"out of Total of \", total, \" : \", errorRate)\n",
    "    return errorRate\n",
    "\n",
    "\n",
    "FACE = \"FACE\"\n",
    "DIGIT = \"DIGIT\"\n",
    "DIR = \"DIR\"\n",
    "HEIGHT = \"HEIGHT\"\n",
    "WIDTH = \"WIDTH\"\n",
    "LABEL = \"LABEL\"\n",
    "PIXELS = \"PIXELS\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"TRAINING OUR MODEL FIRST\")\n",
    "    PERCENT_INCREMENT = 10\n",
    "\n",
    "    perceptron_y = []\n",
    "    bayes_y = []\n",
    "    knn_y = []\n",
    "    dataSetIncrements = []\n",
    "    perceptron_time = []\n",
    "    bayes_time = []\n",
    "    knn_time = []\n",
    "    perceptron_msd=[]\n",
    "    bayes_msd=[]\n",
    "    knn_msd=[]\n",
    "\n",
    "    inp = input(\"Type FACE or DIGIT\")\n",
    "    gridSize = int(input(\"Value of Grid\"))\n",
    "    POSSIBLE_VALUES = [x for x in range(0, gridSize * gridSize + 1)]\n",
    "\n",
    "    map = {\n",
    "        FACE: {\n",
    "            DIR: 'data/facedata', HEIGHT: 68, WIDTH: 61, LABEL: 2, PIXELS: None\n",
    "        },\n",
    "        DIGIT: {\n",
    "            DIR: 'data/digitdata', HEIGHT: 20, WIDTH: 29, LABEL: 10, PIXELS: None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    dataType = map.get(inp)\n",
    "    samples = Samples(dataType.get(DIR))\n",
    "\n",
    "    dataClassifier = DataClassifier(dataType.get(HEIGHT), dataType.get(WIDTH), dataType.get(LABEL),\n",
    "                                    dataType.get(PIXELS), gridSize)\n",
    "    perceptronClassifier = PerceptronClassifier(dataClassifier.FEATURES, dataClassifier.LABELS)\n",
    "\n",
    "    samples.readFiles()\n",
    "\n",
    "    # Extracting Features from the Training Data\n",
    "    dataset = 0\n",
    "    featureValueListForAllTrainingImages, actualLabelForTrainingList = \\\n",
    "        dataClassifier.extractFeatures(samples.train_lines_itr, samples.train_labelsLines_itr)\n",
    "\n",
    "    TOTALDATASET = len(actualLabelForTrainingList)\n",
    "    INCREMENTS = int(TOTALDATASET * PERCENT_INCREMENT / 100)\n",
    "    PERCEPTRON_TIME = {}\n",
    "\n",
    "    # Initialization of Classifiers\n",
    "    perceptronClassifier = PerceptronClassifier(dataClassifier.FEATURES, dataClassifier.LABELS)\n",
    "    naiveBayesClassifier = NaiveBayesClassifier(dataClassifier.FEATURES, dataClassifier.LABELS, POSSIBLE_VALUES)\n",
    "    KNNClassifier = KNN(num_neighbors=20)\n",
    "\n",
    "    featureValueListForAllTestingImages = actualTestingLabelList = []\n",
    "    while dataset < TOTALDATASET:\n",
    "\n",
    "        featureValueList_currentTrainingImages = featureValueListForAllTrainingImages[dataset:dataset + INCREMENTS]\n",
    "        actualLabel_currentTrainingImages = actualLabelForTrainingList[dataset:dataset + INCREMENTS]\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\\n Training ON {0} to {1} data\".format(dataset, dataset + INCREMENTS))\n",
    "        ImageFeatureLabelZipList = zip(featureValueList_currentTrainingImages, actualLabel_currentTrainingImages)\n",
    "\n",
    "        startTimer = time.time()\n",
    "        ''' ####################  TRAINING PHASE FOR PERCEPTRON ############# '''\n",
    "        for featureValueListPerImage, actualLabel in ImageFeatureLabelZipList:\n",
    "            perceptronClassifier.runModel(True, featureValueListPerImage, actualLabel)\n",
    "        endTimer = time.time()\n",
    "\n",
    "        perceptron_time.append(endTimer - startTimer)\n",
    "\n",
    "        startTimer = time.time()\n",
    "        ''' ####################  TRAINING PHASE FOR NAIVE BYES ############# '''\n",
    "        naiveBayesClassifier.constructLabelsProbability(actualLabel_currentTrainingImages)\n",
    "        naiveBayesClassifier.constructFeaturesProbability(featureValueList_currentTrainingImages,\n",
    "                                                          actualLabel_currentTrainingImages,\n",
    "                                                          POSSIBLE_VALUES)\n",
    "        endTimer = time.time()\n",
    "\n",
    "        bayes_time.append(endTimer - startTimer)\n",
    "\n",
    "        ''' ################## NO TRAINING PHASE FOR KNN #################  '''\n",
    "        KNNClassifier.storeTrainingSet(featureValueList_currentTrainingImages, actualLabel_currentTrainingImages)\n",
    "        ''' SIMPLY STORING FOR KNN '''\n",
    "\n",
    "        ''' ####################  TESTING PHASE ############# '''\n",
    "        samples.initTestIters()\n",
    "\n",
    "        print(\"TESTING our model that is TRAINED ON {0} to {1} data\".format(0, dataset + INCREMENTS))\n",
    "\n",
    "        perceptron_errorPrediction = naiveByes_errorPrediction = knn_errorPrediction = total = 0\n",
    "        featureValueListForAllTestingImages, actualTestingLabelList = \\\n",
    "            dataClassifier.extractFeatures(samples.test_lines_itr, samples.test_labelsLines_itr)\n",
    "\n",
    "        for featureValueListPerImage, actualLabel in zip(featureValueListForAllTestingImages, actualTestingLabelList):\n",
    "            perceptron_errorPrediction += perceptronClassifier.runModel(False, featureValueListPerImage, actualLabel)\n",
    "            naiveByes_errorPrediction += naiveBayesClassifier.testModel(featureValueListPerImage, actualLabel)\n",
    "\n",
    "            ''' ####################  TESTING PHASE FOR KNN ############# '''\n",
    "            startTimer = time.time()\n",
    "\n",
    "            knn_errorPrediction += KNNClassifier.test(featureValueListPerImage, actualLabel)\n",
    "\n",
    "            endTimer = time.time()\n",
    "            knn_time.append(endTimer - startTimer)\n",
    "            ''' ####################  TESTING PHASE OVER FOR KNN ############# '''\n",
    "\n",
    "            total += 1\n",
    "\n",
    "        perceptron_error = error(perceptron_errorPrediction, total)\n",
    "        bayes_error = error(naiveByes_errorPrediction, total)\n",
    "        knn_error = error(knn_errorPrediction, total)\n",
    "        perceptron_msd.append(perceptron_error)\n",
    "        bayes_msd.append(bayes_error)\n",
    "        knn_msd.append(knn_error)\n",
    "\n",
    "        perceptron_msd_graph = mean_standard_deviation(perceptron_msd,\"Perceptron\")\n",
    "        bayes_msd_graph = mean_standard_deviation(bayes_msd,\"Bayes\")\n",
    "        knn_msd_graph = mean_standard_deviation(knn_msd,\"KNN\")\n",
    "\n",
    "        dataset += INCREMENTS\n",
    "\n",
    "        dataSetIncrements.append(dataset)\n",
    "        perceptron_y.append(perceptron_error)\n",
    "        bayes_y.append(bayes_error)\n",
    "        knn_y.append(knn_error)\n",
    "\n",
    "    final_array = {\n",
    "        1: [perceptron_y, bayes_y, knn_y],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    final_array2 = {\n",
    "        1: [perceptron_time, bayes_time, knn_time],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    final_array3 = {\n",
    "        1: [perceptron_msd_graph, bayes_msd_graph, knn_msd_graph],\n",
    "        2: [\"Perceptron\", \"Bayes\", \"KNN\"]\n",
    "    }\n",
    "\n",
    "    error = Error()\n",
    "    error.graphplot(dataSetIncrements, final_array.get(1), final_array.get(2), inp) #For error plotting\n",
    "    # error.graphplot(dataSetIncrements, final_array2.get(1), final_array2.get(2), inp) #For time\n",
    "#     error.graphplot(dataSetIncrements, final_array3.get(1), final_array3.get(2), inp) #For mean\n",
    "    # error.graphplot(dataSetIncrements, final_array3.get(1)[1], final_array3.get(2), inp) #For Standard Deviation\n",
    "\n",
    "    samples.closeFiles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "class Error:\n",
    "\n",
    "    def graphplot(self, dataset, errorRateList, type, method):\n",
    "        for i in range(len(errorRateList)):\n",
    "            plt.plot(dataset, errorRateList[i], label=type[i])\n",
    "            plt.xlim(0, dataset[-1] + dataset[-1]/10)\n",
    "            plt.ylim(0, 100)\n",
    "\n",
    "        for i in range(len(errorRateList)):\n",
    "            for data, errorRate in zip(dataset, errorRateList[i]):\n",
    "                pyplot.text(data, errorRate, str(int(errorRate)))\n",
    "\n",
    "        plt.title(method)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
