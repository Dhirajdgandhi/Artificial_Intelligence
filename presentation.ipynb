{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Extraction - Single Pixel\n",
    "\n",
    "Our Input Data contains ['#','+'] as pixel values and we consider a weight of 1 for each of these. Having different values for these characters gives the same result. We transform our input data into binary values depending on the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pixelChars = ['#', '+']\n",
    "\n",
    "def countPixels(self, line):\n",
    "    count = 0\n",
    "\n",
    "    if char in self.pixelChars:\n",
    "        count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Extraction - Grid of Pixels\n",
    "\n",
    "We have considered successive grids of size n*\\n as our features to give a better description of input to our classifier. This gave us amazing result improvements increasing accuracy over 10% for every classifier. We calculate total sum of pixels which are valued 1, for a GRID of size n\\*n and store this as our feature.   \n",
    "\n",
    "We observed the following error rate for varied grid sizes for our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: classifier per grid size results (graph maybe)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We have **FACE** and **DIGIT** datasets. We have combined the Training and Validation Datasets for our Training. The Testing dataset is purely used for Testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classifier Algorithms \n",
    "\n",
    "<br> <hr> <hr> <br>\n",
    "\n",
    "## 1. Perceptron \n",
    "\n",
    "*Perceptron* is a supervised learning algorithm where we train our algorithm from a *labeled* training dataset.\n",
    "\n",
    "Weight matrix is of SHAPE as below and we initialize our weights with random numbers. Every LABEL has a weight array of FEATURES + 1 length. The + 1 is for the bias weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(self, FEATURES, LABELS):\n",
    "    self.SHAPE = (LABELS, FEATURES + 1)  # The +1 is for our w0 weight.\n",
    "    self.weightMatrix = np.zeros(self.SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training :\n",
    "\n",
    "For Traning, we calculate the dot product between the weights and feature values and sum up these. We do this for all the labels and find the probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def runModel(self, isTrain, featureValueList, actualLabel):\n",
    "    for labelWeights in self.weightMatrix:\n",
    "        featureScoreList.append(np.sum(np.dot(labelWeights, featureValueList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The maximum probability is our predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictedLabel = np.argmax(featureScoreList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Weight updation :\n",
    "If the predicted and actual lables do not match, we update the weights by adding/subtracting the feature values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if predictedLabel != actualLabel:\n",
    "    if isTrain:\n",
    "        self.updateWeights(predictedLabel, actualLabel, featureValueList)\n",
    "    else: return 1\n",
    "else: return 0\n",
    "    \n",
    "def updateWeights(self, predictedLabel, actualLabel, featureValueList):\n",
    "    self.weightMatrix[actualLabel] = self.weightMatrix[actualLabel] + featureValueList\n",
    "    self.weightMatrix[predictedLabel] = self.weightMatrix[predictedLabel] - featureValueList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run -i dataclassifier.py --input=FACE --gridSize=3 --smoothingValue=0.001 --classifier=PERCEPTRON  --percentIncrement=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i dataclassifier.py --input=DIGIT --gridSize=3 --smoothingValue=0.001 --classifier=PERCEPTRON  --percentIncrement=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "- Perceptrons have the ability to learn themselves the complex relationships and patterns in the dataset.\n",
    "- We can have any type of input. It does not restrict to use any one datatype as inputs.\n",
    "- If we have a single layer of perceptron, then the training is very quick.\n",
    "- Is really accurate for image processing and character recognition.\n",
    "\n",
    "### **Disadvantages**\n",
    "- A single layer of perceptron cannot train a problem whose solution is a non-linear function.\n",
    "- Multi layer perceptron takes more time to train.\n",
    "- Difficult optimization if we have a lot of local minima/maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br> <hr> <hr> <br>\n",
    "\n",
    "## 2. Naive Bayes Classifier\n",
    "*Naive Bayes Classifier* is a part of the probabilistic classifiers based on Bayes' Theorem. The formula for Bayes' theorem is $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$. There are two assumptions taken in this algorithm.\n",
    "- That the feature is independent.\n",
    "- The importance of every feature is equal.\n",
    "\n",
    "### Initialization\n",
    "2 maps to store our frequency count probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialization of FMAP - FEATURES X LABELS X POSSIBLE_VALUES\n",
    "for featureIndex in range(self, self.FEATURES):\n",
    "    for labelIndex in range(self.LABELS):\n",
    "        for possibleValueIndex in POSSIBLE_VALUES:\n",
    "            self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = 0\n",
    "\n",
    "# Initialization of LMAP - LABELS\n",
    "for labelIndex in range(self, 0, self.LABELS):\n",
    "    self.LabelMap[labelIndex] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training\n",
    "We fill the 2 maps initialized, with our input data features frequency and then calculate the $LOG(P(frequency)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def constructLabelsProbability(self, trainingLabels):\n",
    "    # Storing Frequency\n",
    "    for label in trainingLabels: self.LabelMap[label] += 1\n",
    "\n",
    "    # Calculating probability -> frequency/total -> LOG\n",
    "    for key in self.LabelMap:\n",
    "        probability = self.LabelMap[key] / totalDataset\n",
    "        self.LabelMap[key] = probability\n",
    "\n",
    "def constructFeaturesProbability(self, featureValueListForAllTrainingImages, actualLabelForTrainingList, POSSIBLE_VALUES):\n",
    "\n",
    "    # TRAINING\n",
    "    for label, featureValuesPerImage in zip(actualLabelForTrainingList, featureValueListForAllTrainingImages):\n",
    "        for feature in range(0, self.FEATURES):\n",
    "            self.FeatureMap[feature][label][featureValuesPerImage[feature]] += 1\n",
    "\n",
    "    # Then Converting frequencies to probabilities and then to it's LOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing :\n",
    "\n",
    "- For every label, we calculate the $P(features|label)$ using Bayes' theorem as below. \n",
    "- We calculate in terms of LOGs because multiplying many probabilities together can result in underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predictLabel_GivenFeatures(self, featuresListOfImage):\n",
    "    for label in self.LabelMap:\n",
    "        P_Y = self.LabelMap.get(label)\n",
    "        for feature in range(0, self.FEATURES):\n",
    "            P_features_given_Y += math.log(self.FeatureMap[feature][label][featuresListOfImage[feature]])\n",
    "        \n",
    "        probability = math.log(P_Y, 2) + P_features_given_Y\n",
    "        probabilityPerLabel.append(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabel = np.argmax(probabilityPerLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Laplace Smoothing\n",
    "- If the algorithm does not see any connections between $A$ and $B$ in the formula, it would give $P(A|B) = 0$. \n",
    "- We have used *Laplace Smoothing* to get rid of any probabilities leading to 0.\n",
    "- We experimented with the smoothing value with these values and found the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i dataclassifier.py --input=DIGIT --gridSize=3 --smoothingValue=0.001 --classifier=NAIVEBAYES  --percentIncrement=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run -i dataclassifier.py --input=FACE --gridSize=3 --smoothingValue=0.001 --classifier=NAIVEBAYES  --percentIncrement=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i dataclassifier.py --input=DIGIT --gridSize=3 --smoothingValue=0.001 --classifier=NAIVEBAYES  --percentIncrement=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- It is easy to implement.\n",
    "- Requires just small amount of data to establish the beliefs.\n",
    "- It is less sensitive to lost data.\n",
    "- Speed of training and testing is pretty high\n",
    "\n",
    "**Disadvantages**\n",
    "- We need prior probability.\n",
    "- The assumption that all the features are independent.\n",
    "- If there is a test data that was not seen during training, then the probability of this data would be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br> <hr> <hr> <br>\n",
    "\n",
    "## K-Nearest Neighbors(KNN)\n",
    "\n",
    "*KNN* algorithm predicts on the basis that similar items are together. This uses the classification method to identify in which class the new data belongs. It is also called lazy learning as there is no training. The algorithm learns when the testing is performed. So, during the training time, the algorithm just stores the feature values and the labels of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Training: Actually just Storing the Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "def storeTrainingSet(self, featuresPerImage, labelForImage):\n",
    "    self.features += featuresPerImage\n",
    "    self.labels += labelForImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Currently, we pass the number of neighbors $k=20$ but the value of $k$ depends on the datasize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is the main function of the code. All other classes are imported in this file and are accessed here as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Testing:\n",
    "\n",
    "We use **Eucledian distance** to calculate the distance of the new data from the all the trainingset data points.\n",
    "- Sorts these data points according to the distance\n",
    "- Finds nearest k neighbors\n",
    "- Find the frequent label amongst these neighbors - This label is our predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locate the most similar neighbors\n",
    "def get_neighbors(self, featureForTestImage, labelForTestImage):\n",
    "    \n",
    "    for featureTrainingImage, labelTrainingImage  in zip(self.features, self.labels):\n",
    "        dist = euclidean_distance(featureTrainingImage, labelTrainingImage)\n",
    "        distances.append((featureTrainingImage, labelTrainingImage, dist))\n",
    "\n",
    "    distances.sort(key=lambda tup: tup[2]) # Sort according to the dist\n",
    "    for i in range(self.num_neighbors):\n",
    "        neighbors.append(distances[i][1])\n",
    "    return neighbors\n",
    "\n",
    "neighbors = self.get_neighbors(test_row)\n",
    "predictedLabel = most_frequent(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "**Advantages**\n",
    "- No training period. Learns when testing the data.\n",
    "- Can accept new data flexibly.\n",
    "- Easy to implement.\n",
    "\n",
    "**Disadvantages**\n",
    "- We need to standardize the input to scale them for appropriate predictions.\n",
    "- If some data is missing or the dataset has some error, it can give wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'dataclassifier.py' --input=FACE --gridSize=3 --smoothingValue=0.001 --classifier=PERCEPTRON  --percentIncrement=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}