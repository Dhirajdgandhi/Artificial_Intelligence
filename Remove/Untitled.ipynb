{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron \n",
    "\n",
    "*Perceptron* is a supervised learning algorithm for binary classifiers. It is supervised because we can train this algorithm from a training dataset and test it out on the testing data. It gives results in a linear function. Perceptrons are just a single layer Neural Network. If we add many perceptrons taking care of different functionalities, we can create a multi layer Perceptron. In the early days, it was difficult to classify some problems in as a linear function. So they were not considered as a serious solution to Artificial Intelligence. But, when people experimented with adding more layers to the perceptron, they found that problems which cannot be classified as a linear function, can now be solved. One example of this is the **XOR** solution. This XOR problem cannot be implemented in a single layer of perceptron as the results are not linearly separable. Therefore, people tried to implement multi-layer perceptron which got the required result. Hence, it was the advent of Perceptron and Artificial Intelligence.\n",
    "\n",
    "In this algorithm, we create a feature function. This function takes features from the image according to the function specified. *For our implementation*, we ask the user to input the number of grids they want to compute on. In general, if the grid size is greater than or equal to 3\\*3, the accuracy is higher. Also, we take input from the user the dataset to run these algorithms on. Users can enter **FACE** or **DIGIT** to run the algorithm on face or digit datasets respectively.\n",
    "\n",
    "Initially, we take random weights for features of each label and compute the score of feature of a class(for digits, 6 or 9 & for face, face or no face). It can be mathematically be written as $score(f,y) = \\sum_i f_i w_i^y$. This equation means we calculate the score for feature vector *f* for a particular class *y* which equals to sum of the multiplication of all the features vectors with its weight of its class. \n",
    "\n",
    "We update the weights by calculating the maximum of the the score it gives for a label from the individual features. We can write it as $ y^{*} = arg max score(f,y^{**}) $ . Then after computing each feature and checking for the actual label, we predict if the result is accurate. For example, if the label is *6*, and we get a prediction of *9*, then, in this case, we decrease the values of weights for the label 9 as $w^9 = w^9 - f$ where f is some predefined value which we will subtract from the current weight. But for the weights of 6, we will add f to its current weight as $w^6 = w^6 + f$. Finally, the perceptron algorithm would tune the weight according to the actual label during the training and try to get the accurate result in the testing data.\n",
    "\n",
    "**Advantages**\n",
    "- Perceptrons have the ability to learn themselves the complex relationships and patterns in the dataset.\n",
    "- We can have any type of input. It does not restrict to use any one datatype as inputs.\n",
    "- If we have a single layer of perceptron, then the training is very quick.\n",
    "- Is really accurate for image processing and character recognition.\n",
    "\n",
    "**Disadvantages**\n",
    "- A single layer of perceptron cannot train a problem whose solution is a non-linear function.\n",
    "- Multi layer perceptron takes more time to train.\n",
    "- Difficult optimization if we have a lot of local minima/maxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PerceptronClassifier:\n",
    "    def __init__(self, FEATURES, LABELS):\n",
    "        self.SHAPE = (LABELS, FEATURES + 1)  # The +1 is for our w0 weight.\n",
    "        self.weightMatrix = np.zeros(self.SHAPE)\n",
    "\n",
    "    def updateWeights(self, predictedLabel, actualLabel, featureValueList):\n",
    "        # print(\"Updating Weights\")\n",
    "        self.weightMatrix[actualLabel] = self.weightMatrix[actualLabel] + featureValueList\n",
    "        self.weightMatrix[predictedLabel] = self.weightMatrix[predictedLabel] - featureValueList\n",
    "        # print(weightMatrix[actualLabel, :])\n",
    "        # print(weightMatrix[predictedLabel, :])\n",
    "\n",
    "    def runModel(self, isTrain, featureValueList, actualLabel):\n",
    "        featureScoreList = []\n",
    "        featureValueList = [1] + featureValueList # The [1] + is to accommodate the bias weight - w0\n",
    "        for labelWeights in self.weightMatrix:\n",
    "            featureScoreList.append(np.sum(np.dot(labelWeights, featureValueList)))\n",
    "\n",
    "        # print(\"Feature Score List :\", featureScoreList)\n",
    "        predictedLabel = np.argmax(featureScoreList)\n",
    "\n",
    "        if predictedLabel != actualLabel:\n",
    "            # print(predictedLabel, \" \", actualLabel)\n",
    "            if isTrain:\n",
    "                self.updateWeights(predictedLabel, actualLabel, featureValueList)\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def initWeightMatrix(self):\n",
    "        self.weightMatrix = np.zeros(self.SHAPE)  # Randomized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "*Naive Bayes Classifier* is a part of the probabilistic classifiers based on Bayes' Theorem. The formula for Bayes' theorem is $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$. There are two assumptions taken in this algorithm.\n",
    "- That the feature is independent.\n",
    "- The importance of every feature is equal.\n",
    "When we have a label *X*, the naive bayes tries to model it using joint distribution from the formula mentioned above. For every labels, we have features dedicated for them as a group like $ (f_1, f_2, ..., f_n) $. We have a  formula given for calculating the joing probability. $ (f_1, f_2, ..., f_n, X) = P(X) \\prod_i P(F_i|X) $. As we had done in the perceptron algorithm, we again calculate the argmax of the features from the probabilities and predict the label from the set of inputs given. \n",
    "\n",
    "Another thing for the prediction is smoothing. In real life, we do not want to set the probabilities of any term to be 0. But, if the algorithm does not see any connections between $A$ and $B$ in the formula, it would give $P(A|B) = 0$. This is not acceptable in real world predications and as a result, we use *Laplace Smoothing* to get rid of any probabilities leading to 0. We have used smoothing value to be 0.001 as we do not want to vary the results very much.\n",
    "\n",
    "When we want the algorithm to learn, we use *Odds Ratio*. For every feature and every pair of classes we have, we check each one of them to increase the belief in one class or the other. \n",
    "\n",
    "**Advantages**\n",
    "- It is easy to implement.\n",
    "- Requires just small amount of data to establish the beliefs.\n",
    "- It is less sensitive to lost data.\n",
    "- Speed of training and testing is pretty high\n",
    "\n",
    "**Disadvantages**\n",
    "- We need prior probability.\n",
    "- The assumption that all the features are independent.\n",
    "- If there is a test data that was not seen during training, then the probability of this data would be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    P_A_GIVEN_B = 'P(A|B)'\n",
    "    P_B_GIVEN_A = 'P(B|A)'\n",
    "    P_A = 'P(A)'\n",
    "    P_B = 'P(B)'\n",
    "\n",
    "    # Smotthing\n",
    "    kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "\n",
    "    def __init__(self, FEATURES, LABELS, POSSIBLE_VALUES, k_value):\n",
    "        self.LabelMap = {}\n",
    "        self.FeatureMap = {}\n",
    "        self.FEATURES = FEATURES\n",
    "        self.LABELS = LABELS\n",
    "        self.K = k_value\n",
    "\n",
    "        # Initialization of FMAP - FEATURES X LABELS X POSSIBLE_VALUES\n",
    "        for featureIndex in range(self.FEATURES):\n",
    "            self.FeatureMap[featureIndex] = {}\n",
    "            for labelIndex in range(self.LABELS):\n",
    "                self.FeatureMap[featureIndex][labelIndex] = {}\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = 0\n",
    "\n",
    "        # Initialization\n",
    "        for labelIndex in range(0, self.LABELS):\n",
    "            self.LabelMap[labelIndex] = 0\n",
    "\n",
    "    def P_A_given_B(self, map):\n",
    "        result = ( map.get(NaiveBayesClassifier.P_B_GIVEN_A) * map.get(NaiveBayesClassifier.P_A) )\\\n",
    "                 / map.get(NaiveBayesClassifier.P_B)\n",
    "        return result\n",
    "\n",
    "    # Constructing Labels probability\n",
    "    # PRIOR DISTRIBUTION OVER LABELS #\n",
    "    def constructLabelsProbability(self, trainingLabels):\n",
    "        totalDataset = len(trainingLabels)\n",
    "\n",
    "        # Storing Frequency\n",
    "        for label in trainingLabels:\n",
    "            self.LabelMap[label] += 1\n",
    "\n",
    "        # Calculating probability -> frequency/total -> LOG\n",
    "        for key in self.LabelMap:\n",
    "            probability = self.LabelMap[key] / totalDataset\n",
    "            self.LabelMap[key] = probability\n",
    "\n",
    "    def constructFeaturesProbability(self, featureValueListForAllTrainingImages, actualLabelForTrainingList, POSSIBLE_VALUES):\n",
    "\n",
    "        # TRAINING\n",
    "        for label, featureValuesPerImage in zip(actualLabelForTrainingList, featureValueListForAllTrainingImages):\n",
    "            for feature in range(0, self.FEATURES):\n",
    "                self.FeatureMap[feature][label][featureValuesPerImage[feature]] += 1\n",
    "\n",
    "        # Converting frequencies to probabilities to it's LOG\n",
    "        for featureIndex in range(self.FEATURES):\n",
    "            for labelIndex in range(self.LABELS):\n",
    "                sum = 0\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    sum += self.FeatureMap.get(featureIndex).get(labelIndex).get(possibleValueIndex) + self.K\n",
    "                for possibleValueIndex in POSSIBLE_VALUES:\n",
    "                    probability = (self.FeatureMap.get(featureIndex).get(labelIndex).get(possibleValueIndex) + self.K) / sum\n",
    "                    self.FeatureMap[featureIndex][labelIndex][possibleValueIndex] = probability\n",
    "\n",
    "    def predictLabel_GivenFeatures(self, featuresListOfImage):\n",
    "        probabilityPerLabel = []\n",
    "        for label in self.LabelMap:\n",
    "            # P(Y=label|features)\n",
    "            P_Y = self.LabelMap.get(label)\n",
    "            P_features_given_Y = 0\n",
    "            for feature in range(0, self.FEATURES):\n",
    "                P_features_given_Y += math.log(self.FeatureMap[feature][label][featuresListOfImage[feature]])\n",
    "            probability = math.log(P_Y, 2) + P_features_given_Y\n",
    "            probabilityPerLabel.append(probability)\n",
    "\n",
    "        predictedLabel = np.argmax(probabilityPerLabel)\n",
    "        return predictedLabel\n",
    "\n",
    "    def testModel(self, featuresListOfImage, actualLabel):\n",
    "        predictedLabel = self.predictLabel_GivenFeatures(featuresListOfImage)\n",
    "        if predictedLabel != actualLabel:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors(KNN)\n",
    "\n",
    "*KNN* algorithm predicts on the basis that similar items are together. This uses the classification method to identify in which class the new data belongs. It is also called lazy learning as there is no training. The algorithm learns when the testing is performed. So, during the training time, the algorithm just stores the feature values and the labels of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Actually just Storing the Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeTrainingSet(self, featuresPerImage, labelForImage):\n",
    "    self.features += featuresPerImage\n",
    "    self.labels += labelForImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we pass the number of neighbors $k=20$ but the value of $k$ depends on the datasize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing:\n",
    "\n",
    "We use **Eucledian distance** to calculate the distance of the new data from the all the trainingset data points.\n",
    "- Sorts these data points according to the distance\n",
    "- Finds nearest k points and then see the most frequent labe that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the most similar neighbors\n",
    "def get_neighbors(self, featureForTestImage, labelForTestImage):\n",
    "    \n",
    "    for featureTrainingImage, labelTrainingImage  in zip(self.features, self.labels):\n",
    "        dist = euclidean_distance(featureTrainingImage, labelTrainingImage)\n",
    "        distances.append((featureTrainingImage, labelTrainingImage, dist))\n",
    "\n",
    "    distances.sort(key=lambda tup: tup[2]) # Sort according to the dist\n",
    "    for i in range(self.num_neighbors):\n",
    "        neighbors.append(distances[i][1])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- No training period. Learns when testing the data.\n",
    "- Can accept new data flexibly.\n",
    "- Easy to implement.\n",
    "\n",
    "**Disadvantages**\n",
    "- Accuracy is small with large datasets.\n",
    "- We need to standardize the input to scale them for appropriate predictions.\n",
    "- If some data is missing or the dataset has some error, it can give wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Volumes/Files Documents/Academics and Projects/Engineering/7th Sem/7th sem/artificial intelligence/Rutgers/FinalProject/dataclassifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mfeatureValueListForAllTrainingImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactualLabelForTrainingList\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdataClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lines_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labelsLines_itr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mtotalDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactualLabelForTrainingList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Files Documents/Academics and Projects/Engineering/7th Sem/7th sem/artificial intelligence/Rutgers/FinalProject/dataclassifier.py\u001b[0m in \u001b[0;36mextractFeatures\u001b[0;34m(self, lines_itr, labelsLines_itr)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mimageLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mfeatureValueListPerImage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitImageLineFeaturesIntoGridFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageLinesList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mtotalImages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Files Documents/Academics and Projects/Engineering/7th Sem/7th sem/artificial intelligence/Rutgers/FinalProject/dataclassifier.py\u001b[0m in \u001b[0;36msplitImageLineFeaturesIntoGridFeatures\u001b[0;34m(self, imageLinesList, gridSize)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0msum1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0msum1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mheight_new_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrowIndex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mfeatureListForImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run -i dataclassifier.py --input=FACE --gridSize=3 --smoothingValue=0.001 --classifier=PERCEPTRON  --percentIncrement=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}